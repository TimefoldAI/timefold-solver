= Enterprise Edition
:page-aliases: partitioned-search/partitioned-search.adoc
:doctype: book
:sectnums:
:icons: font

Timefold Solver Enterprise Edition is a commercial product that offers additional features,
such as <<nearbySelection,nearby selection>> and <<multithreadedSolving,multi-threaded solving>>.
These features are essential to scale out to huge datasets.

Unlike Timefold Solver Community Edition, the Enterprise Edition is not open-source.
You are allowed to use Timefold Solver Enterprise Edition for evaluation and development.
Please https://timefold.ai/contact[contact Timefold]
to obtain the credentials necessary to start your evaluation.

TIP: Looking for quicker time-to-value?
Timefold offers https://docs.timefold.ai/[pre-built, fully tuned optimization models], no constraint building required.
Just plug into our API and start optimizing immediately.

For a high-level overview of the differences between Timefold offerings,
see https://timefold.ai/pricing[Timefold Pricing].

[#switchToEnterpriseEdition]
== Switch to Enterprise Edition

// Uncomment the following when ready to enable the Enterprise license
// =============================
// To switch from Timefold Solver Community Edition to Enterprise Edition,
// https://timefold.ai/contact[contact us first] to obtain an Enterprise License.
// This license is represented by a file named `timefold-solver-enterprise-license.pem`,
// which you can introduce to your project by one of the following methods:
//
// * Place the file in the user's home directory.
// * Store the path to the file in `TIMEFOLD_ENTERPRISE_LICENSE` system property.
// * Place the file in the root of your application classpath.
//
// Having done that, reference the Enterprise Edition Maven repository in your project:
// =============================
// Also delete the next line of text.

First reference the Enterprise Edition Maven repository in your project:

[tabs]
====
Maven::
+
--
Add the following repository to your `pom.xml`:

[source,xml,options="nowrap"]
----
<project>
  ...
  <repositories>
    <repository>
      <id>timefold-solver-enterprise</id>
      <name>Timefold Solver Enterprise Edition</name>
      <url>https://timefold.jfrog.io/artifactory/releases/</url>
    </repository>
  </repositories>
  ...
</project>
----

Then create `.m2/settings.xml` in your home directory with the following content:

[source,xml,options="nowrap"]
----
<settings>
  ...
  <servers>
    <server>
      <!-- Replace "my_username" and "my_password" with credentials obtained from a Timefold representative. -->
      <id>timefold-solver-enterprise</id>
      <username>my_username</username>
      <password>my_password</password>
    </server>
  </servers>
  ...
</settings>
----

See https://maven.apache.org/settings.html[Settings Reference] for more information on Maven settings.
--

Gradle::
+
--
Add the following in your `build.gradle`:

[source,groovy,options="nowrap"]
----
repositories {
  mavenCentral()
  maven {
    url "https://timefold.jfrog.io/artifactory/releases/"
    credentials { // Replace "my_username" and "my_password" with credentials obtained from a Timefold representative.
        username "my_username"
        password "my_password"
    }
    authentication {
        basic(BasicAuthentication)
    }
  }
}
----
--
====

Finally, having done all of the above,
replace references to Community Edition artifacts by their Enterprise Edition counterparts
as shown in the following table:

|===
|Community Edition|Enterprise Edition

|`ai.timefold.solver:timefold-solver-bom`
|`ai.timefold.solver.enterprise:timefold-solver-enterprise-bom`

|`ai.timefold.solver:timefold-solver-core`
|`ai.timefold.solver.enterprise:timefold-solver-enterprise-core`

|`ai.timefold.solver:timefold-solver-quarkus`
|`ai.timefold.solver.enterprise:timefold-solver-enterprise-quarkus`

|`ai.timefold.solver:timefold-solver-spring-boot-starter`
|`ai.timefold.solver.enterprise:timefold-solver-enterprise-spring-boot-starter`
|===


[#enterpriseEditionFeatures]
== Features of Enterprise Edition

The following features are only available in Timefold Solver Enterprise Edition:

* <<nearbySelection,Nearby selection>>,
* <<multithreadedIncrementalSolving,multi-threaded incremental solving>>,
* <<partitionedSearch,partitioned search>>,
* <<automaticNodeSharing,automatic node sharing>>,
* and <<throttlingBestSolutionEvents, throttling best solution events>>.


[#nearbySelection]
=== Nearby selection

[NOTE]
====
This feature is a commercial feature of Timefold Solver Enterprise Edition.
It is not available in the Community Edition.
====

In some use cases (such as TSP and VRP, but also in other cases),
changing entities to nearby values or swapping nearby entities leads to better results faster.

image::enterprise-edition/nearbySelectionMotivation.png[align="center"]

This can *heavily increase scalability* and improve solution quality:

image::enterprise-edition/nearbySelectionValueProposition.png[align="center"]

Nearby selection increases the probability of selecting an entity or value which is nearby to the first entity being moved in that move.

image::enterprise-edition/nearbySelectionRandomDistribution.png[align="center"]

The distance between two entities or values is domain specific.
Therefore, implement the `NearbyDistanceMeter` interface:

[tabs]
====
Java::
+
[source,java,options="nowrap"]
----
public interface NearbyDistanceMeter<Origin_, Desination_> {

    double getNearbyDistance(Origin_ origin, Destination_ destination);

}
----
====

In a nutshell, when nearby selection is used in a list move selector,
`Origin_` is always a planning value (for example `Customer`)
but `Destination_` can be either a planning value or a planning entity.
That means that in VRP the distance meter must be able to handle both `Customer` and `Vehicle` as the `Destination_` argument:

[tabs]
====
Java::
+
[source,java,options="nowrap"]
----
public class CustomerNearbyDistanceMeter implements NearbyDistanceMeter<Customer, LocationAware> {

    public double getNearbyDistance(Customer origin, LocationAware destination) {
        return origin.getDistanceTo(destination);
    }

}
----
====

[NOTE]
====
`NearbyDistanceMeter` implementations are expected to be stateless.
The solver may choose to reuse them in different contexts.

The Nearby configuration is not enabled for the Construction Heuristics
because the method will analyze all possible moves.
Adding Nearby in this situation would only result in unnecessary costs
involving the generation of the distance matrix and sorting operations without taking advantage of the feature.
====

==== Nearby selection with a list variable

To quickly configure nearby selection with a planning list variable,
add `nearbyDistanceMeterClass` element to your configuration file.
The following enables nearby selection with a list variable
for the local search:

[source,xml,options="nowrap"]
----
<?xml version="1.0" encoding="UTF-8"?>
<solver xmlns="https://timefold.ai/xsd/solver">
    ...
    <nearbyDistanceMeterClass>org.acme.vehiclerouting.domain.solver.nearby.CustomerNearbyDistanceMeter</nearbyDistanceMeterClass>
    ...
</solver>
----

By default, the following move selectors are included:
xref:optimization-algorithms/move-selector-reference.adoc#changeMoveSelector[Change],
xref:optimization-algorithms/move-selector-reference.adoc#swapMoveSelector[Swap],
Change with Nearby,
Swap with Nearby,
and xref:optimization-algorithms/move-selector-reference.adoc#kOptListMoveSelector[2-OPT] with Nearby.

===== Advanced configuration for local search

To customize the move selectors,
add a `nearbySelection` element in the `destinationSelector`, `valueSelector` or `subListSelector`
and use xref:optimization-algorithms/move-selector-reference.adoc#mimicSelection[mimic selection]
to specify which destination, value, or subList should be nearby the selection.

[source,xml,options="nowrap"]
----
    <unionMoveSelector>
      <listChangeMoveSelector>
        <valueSelector id="valueSelector1"/>
        <destinationSelector>
          <nearbySelection>
            <originValueSelector mimicSelectorRef="valueSelector1"/>
            <nearbyDistanceMeterClass>org.acme.vehiclerouting.domain.solver.nearby.CustomerNearbyDistanceMeter</nearbyDistanceMeterClass>
          </nearbySelection>
        </destinationSelector>
      </listChangeMoveSelector>
      <listSwapMoveSelector>
        <valueSelector id="valueSelector2"/>
        <secondaryValueSelector>
          <nearbySelection>
            <originValueSelector mimicSelectorRef="valueSelector2"/>
            <nearbyDistanceMeterClass>org.acme.vehiclerouting.domain.solver.nearby.CustomerNearbyDistanceMeter</nearbyDistanceMeterClass>
          </nearbySelection>
        </secondaryValueSelector>
      </listSwapMoveSelector>
      <subListChangeMoveSelector>
        <selectReversingMoveToo>true</selectReversingMoveToo>
        <subListSelector id="subListSelector3"/>
        <destinationSelector>
          <nearbySelection>
            <originSubListSelector mimicSelectorRef="subListSelector3"/>
            <nearbyDistanceMeterClass>org.acme.vehiclerouting.domain.solver.nearby.CustomerNearbyDistanceMeter</nearbyDistanceMeterClass>
          </nearbySelection>
        </destinationSelector>
      </subListChangeMoveSelector>
      <subListSwapMoveSelector>
        <selectReversingMoveToo>true</selectReversingMoveToo>
        <subListSelector id="subListSelector4"/>
        <secondarySubListSelector>
          <nearbySelection>
            <originSubListSelector mimicSelectorRef="subListSelector4"/>
            <nearbyDistanceMeterClass>org.acme.vehiclerouting.domain.solver.nearby.CustomerNearbyDistanceMeter</nearbyDistanceMeterClass>
          </nearbySelection>
        </secondarySubListSelector>
      </subListSwapMoveSelector>
    </unionMoveSelector>
----

==== Power-tweaking distribution type

The solver allows you to tweak the distribution type of the nearby selection,
or how likely are the nearest elements to be selected based on their distance from the current.

[NOTE]
====
Only tweak the default settings if you are prepared
to back your choices by extensive xref:using-timefold-solver/benchmarking-and-tweaking.adoc#benchmarker[benchmarking].
====

The following ``NearbySelectionDistributionType``s are supported:

* `PARABOLIC_DISTRIBUTION` (default): Nearest elements are selected with a higher probability.
+
[source,xml,options="nowrap"]
----
  <nearbySelection>
    <parabolicDistributionSizeMaximum>80</parabolicDistributionSizeMaximum>
  </nearbySelection>
----
+
A `distributionSizeMaximum` parameter should not be 1 because if the nearest is already the planning value of the current entity,
then the only move that is selectable is not doable.
To allow every element to be selected regardless of the number of entities,
only set the distribution type (so without a `distributionSizeMaximum` parameter):
+
[source,xml,options="nowrap"]
----
  <nearbySelection>
    <nearbySelectionDistributionType>PARABOLIC_DISTRIBUTION</nearbySelectionDistributionType>
  </nearbySelection>
----
* ``BLOCK_DISTRIBUTION``: Only the n nearest are selected, with an equal probability. For example, select the 20 nearest:
+
[source,xml,options="nowrap"]
----
  <nearbySelection>
    <blockDistributionSizeMaximum>20</blockDistributionSizeMaximum>
  </nearbySelection>
----
* ``LINEAR_DISTRIBUTION``: Nearest elements are selected with a higher probability. The probability decreases linearly.
+
[source,xml,options="nowrap"]
----
  <nearbySelection>
    <linearDistributionSizeMaximum>40</linearDistributionSizeMaximum>
  </nearbySelection>
----
* ``BETA_DISTRIBUTION``: Selection according to a beta distribution. Slows down the solver significantly.
+
[source,xml,options="nowrap"]
----
  <nearbySelection>
    <betaDistributionAlpha>1</betaDistributionAlpha>
    <betaDistributionBeta>5</betaDistributionBeta>
  </nearbySelection>
----


[#multithreadedSolving]
[#enterpriseMultithreadedSolving]
=== Multi-threaded solving

Multi-threaded solving is a term
which encapsulates several features that allow Timefold Solver to run on multi-core machines.
Timefold Solver Enterprise Edition makes multi-threaded solving more powerful by introducing
<<multithreadedIncrementalSolving,multi-threaded incremental solving>> and
<<partitionedSearch,partitioned search>>.

For a primer on multi-threaded solving in general, see xref:using-timefold-solver/running-the-solver.adoc#multithreadedSolving[Multi-threaded solving].

[#multithreadedIncrementalSolving]
==== Multi-threaded incremental solving

[NOTE]
====
This feature is a commercial feature of Timefold Solver Enterprise Edition.
It is not available in the Community Edition.
====

With this feature, the solver can run significantly faster,
getting you the right solution earlier.
It has been designed to speed up the solver in cases where move evaluation is the bottleneck.
This typically happens when the constraints are computationally expensive,
or when the dataset is large.

- The sweet spot for this feature is when the move evaluation speed is up to 10 thousand per second.
In this case, we have observed the algorithm to scale linearly with the number of move threads.
Every additional move thread will bring a speedup,
albeit with diminishing returns.
- For move evaluation speeds on the order of 100 thousand per second,
the algorithm no longer scales linearly,
but using 4 to 8 move threads may still be beneficial.
- For even higher move evaluation speeds,
the feature does not bring any benefit.
At these speeds, move evaluation is no longer the bottleneck.
If the solver continues to underperform,
perhaps you're suffering from xref:constraints-and-score/performance.adoc#scoreTrap[score traps]
or you may benefit from xref:optimization-algorithms/overview.adoc#customMoves[custom moves]
to help the solver escape local optima.

[NOTE]
====
These guidelines are strongly dependent on move selector configuration,
size of the dataset and performance of individual constraints.
We recommend you benchmark your use case
to determine the optimal number of move threads for your problem.
====

===== Enabling multi-threaded incremental solving

Enable multi-threaded incremental solving
by xref:using-timefold-solver/modeling-planning-problems.adoc#planningId[adding a `@PlanningId` annotation]
on every planning entity class and planning value class.
Then configure a `moveThreadCount`:

[tabs]
====
Quarkus::
+
--
Add the following to your `application.properties`:

[source,properties]
----
quarkus.timefold.solver.move-thread-count=AUTO
----
--
Spring::
+
--
Add the following to your `application.properties`:

[source,properties]
----
timefold.solver.move-thread-count=AUTO
----
--
Java::
+
--
Use the `SolverConfig` class:

[source,java,options="nowrap"]
----
SolverConfig solverConfig = new SolverConfig()
    ...
    .withMoveThreadCount("AUTO");
----
--
XML::
+
--
Add the following to your `solverConfig.xml`:
[source,xml,options="nowrap"]
----
<solver xmlns="https://timefold.ai/xsd/solver" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="https://timefold.ai/xsd/solver https://timefold.ai/xsd/solver/solver.xsd">

    ...
    <moveThreadCount>AUTO</moveThreadCount>
    ...

</solver>
----
--
====

Setting `moveThreadCount` to `AUTO` allows Timefold Solver to decide how many move threads to run in parallel.
This formula is based on experience and does not hog all CPU cores on a multi-core machine.

A `moveThreadCount` of `4` xref:integration/integration.adoc#sizingHardwareAndSoftware[saturates almost 5 CPU cores].
the 4 move threads fill up 4 CPU cores completely
and the solver thread uses most of another CPU core.

The following ``moveThreadCount``s are supported:

* `NONE` (default): Don't run any move threads. Use the single threaded code.
* ``AUTO``: Let Timefold Solver decide how many move threads to run in parallel.
On machines or containers with little or no CPUs, this falls back to the single threaded code.
* Static number: The number of move threads to run in parallel.

It is counter-effective to set a `moveThreadCount`
that is higher than the number of available CPU cores,
as that will slow down the move evaluation speed.

[IMPORTANT]
====
In cloud environments where resource use is billed by the hour,
consider the trade-off between cost of the extra CPU cores needed and the time saved.
Compute nodes with higher CPU core counts are typically more expensive to run
and therefore you may end up paying more for the same result,
even though the actual compute time needed will be less.
====

[NOTE]
====
Multi-threaded solving is _still reproducible_, as long as the resolved `moveThreadCount` is stable.
A run of the same solver configuration on 2 machines with a different number of CPUs,
is still reproducible, unless the `moveThreadCount` is set to `AUTO` or a function of `availableProcessorCount`.
====

===== Advanced configuration

There are additional parameters you can supply to your `solverConfig.xml`:

[source,xml,options="nowrap"]
----
<solver xmlns="https://timefold.ai/xsd/solver" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="https://timefold.ai/xsd/solver https://timefold.ai/xsd/solver/solver.xsd">
  <moveThreadCount>4</moveThreadCount>
  <threadFactoryClass>...MyAppServerThreadFactory</threadFactoryClass>
  ...
</solver>
----

To run in an environment that doesn't like arbitrary thread creation,
use `threadFactoryClass` to plug in a <<customThreadFactory,custom thread factory>>.


[#partitionedSearch]
==== Partitioned search

[NOTE]
====
This feature is a commercial feature of Timefold Solver Enterprise Edition.
It is not available in the Community Edition.
====

[#partitionedSearchAlgorithm]
===== Algorithm description

It is often more efficient to partition large data sets (usually above 5000 planning entities)
into smaller pieces and solve them separately.
Partition Search is <<multithreadedSolving,multi-threaded>>,
so it provides a performance boost on multi-core machines due to higher CPU utilization.
Additionally, even when only using one CPU, it finds an initial solution faster,
because the search space sum of a partitioned Construction Heuristic is far less than its non-partitioned variant.

However, **partitioning does lead to suboptimal results**, even if the pieces are solved optimally, as shown below:

image::enterprise-edition/mapReduceIsTerribleForTsp.png[align="center"]

It effectively trades a short term gain in solution quality for long term loss.
One way to compensate for this loss,
is to run a non-partitioned Local Search after the Partitioned Search phase.

[NOTE]
====
Not all use cases can be partitioned.
Partitioning only works for use cases where the planning entities and value ranges can be split into n partitions,
without any of the constraints crossing boundaries between partitions.
====

[#partitionedSearchConfiguration]
===== Configuration

Simplest configuration:

[source,xml,options="nowrap"]
----
  <partitionedSearch>
    <solutionPartitionerClass>...MyPartitioner</solutionPartitionerClass>
  </partitionedSearch>
----

Also xref:using-timefold-solver/modeling-planning-problems.adoc#planningId[add a `@PlanningId` annotation]
on every planning entity class and planning value class.
There are several ways to <<partitioningASolution,partition a solution>>.

Advanced configuration:

[source,xml,options="nowrap"]
----
  <partitionedSearch>
    ...
    <solutionPartitionerClass>...MyPartitioner</solutionPartitionerClass>
    <runnablePartThreadLimit>4</runnablePartThreadLimit>

    <constructionHeuristic>...</constructionHeuristic>
    <localSearch>...</localSearch>
  </partitionedSearch>
----

The `runnablePartThreadLimit` allows limiting CPU usage to avoid hanging your machine, see below.

To run in an environment that doesn't like arbitrary thread creation,
plug in a <<customThreadFactory,custom thread factory>>.

[IMPORTANT]
====
A xref:using-timefold-solver/running-the-solver.adoc#logging[logging level] of `debug` or `trace` causes congestion in multi-threaded Partitioned Search
and slows down the xref:constraints-and-score/performance.adoc#moveEvaluationSpeed[move evaluation speed].
====

Just like a `<solver>` element,
the `<partitionedSearch>` element can contain one or more xref:optimization-algorithms/overview.adoc#solverPhase[phases].
Each of those phases will be run on each partition.

A common configuration is to first run a Partitioned Search phase
(which includes a Construction Heuristic and a Local Search)
followed by a non-partitioned Local Search phase:

[source,xml,options="nowrap"]
----
  <partitionedSearch>
    <solutionPartitionerClass>...MyPartitioner</solutionPartitionerClass>

    <constructionHeuristic/>
    <localSearch>
      <termination>
        <diminishedReturns />
      </termination>
    </localSearch>
  </partitionedSearch>
  <localSearch/>
----


[#partitioningASolution]
===== Partitioning a solution


[#customSolutionPartitioner]
====== Custom `SolutionPartitioner`

To use a custom `SolutionPartitioner`, configure one on the Partitioned Search phase:

[source,xml,options="nowrap"]
----
  <partitionedSearch>
    <solutionPartitionerClass>...MyPartitioner</solutionPartitionerClass>
  </partitionedSearch>
----

Implement the `SolutionPartitioner` interface:

[source,java,options="nowrap"]
----
public interface SolutionPartitioner<Solution_> {

    List<Solution_> splitWorkingSolution(Solution_ workingSolution, Integer runnablePartThreadLimit);

}
----

The `size()` of the returned `List` is the `partCount` (the number of partitions).
This can be decided dynamically, for example, based on the size of the non-partitioned solution.
The `partCount` is unrelated to the `runnablePartThreadLimit`.

To configure values of a `SolutionPartitioner` dynamically in the solver configuration
(so the xref:using-timefold-solver/benchmarking-and-tweaking.adoc#benchmarker[Benchmarker] can tweak those parameters),
add the `solutionPartitionerCustomProperties` element and use xref:using-timefold-solver/configuration.adoc#customPropertiesConfiguration[custom properties]:

[source,xml,options="nowrap"]
----
  <partitionedSearch>
    <solutionPartitionerClass>...MyPartitioner</solutionPartitionerClass>
    <solutionPartitionerCustomProperties>
      <property name="myPartCount" value="8"/>
      <property name="myMinimumProcessListSize" value="100"/>
    </solutionPartitionerCustomProperties>
  </partitionedSearch>
----

[#runnablePartThreadLimit]
===== Runnable part thread limit

When running a multi-threaded solver, such as Partitioned Search, CPU power can quickly become a scarce resource,
which can cause other processes or threads to hang or freeze.
However, Timefold Solver has a system to prevent CPU starving of
other processes (such as an SSH connection in production or your IDE in development)
or other threads (such as the servlet threads that handle REST requests).

As explained in xref:integration/integration.adoc#sizingHardwareAndSoftware[sizing hardware and software],
each solver (including each child solver) does no IO during `solve()` and therefore saturates one CPU core completely.
In Partitioned Search, every partition always has its own thread, called a part thread.
It is impossible for two partitions to share a thread,
because of xref:optimization-algorithms/overview.adoc#asynchronousTermination[asynchronous termination]:
the second thread would never run.
Every part thread will try to consume one CPU core entirely, so if there are more partitions than CPU cores,
this will probably hang the system.
`Thread.setPriority()` is often too weak to solve this hogging problem, so another approach is used.

The `runnablePartThreadLimit` parameter specifies how many part threads are runnable at the same time.
The other part threads will temporarily block and therefore will not consume any CPU power.
*This parameter basically specifies how many CPU cores are donated to Timefold Solver.*
All part threads share the CPU cores in a round-robin manner
to consume (more or less) the same number of CPU cycles:

image::enterprise-edition/partitionedSearchThreading.png[align="center"]

The following `runnablePartThreadLimit` options are supported:

* `UNLIMITED`: Allow Timefold Solver to occupy all CPU cores, do not avoid hogging.
Useful if a no hogging CPU policy is configured on the OS level.
* `AUTO` (default): Let Timefold Solver decide how many CPU cores to occupy. This formula is based on experience.
It does not hog all CPU cores on a multi-core machine.
* Static number: The number of CPU cores to consume. For example:
+
[source,xml,options="nowrap"]
----
<runnablePartThreadLimit>2</runnablePartThreadLimit>
----

[WARNING]
====
If the `runnablePartThreadLimit` is equal to or higher than the number of available processors,
the host is likely to hang or freeze,
unless there is an OS specific policy in place to avoid Timefold Solver from hogging all the CPU processors.
====


[#customThreadFactory]
==== Custom thread factory (WildFly, GAE, ...)

The `threadFactoryClass` allows to plug in a custom `ThreadFactory` for environments
where arbitrary thread creation should be avoided,
such as most application servers (including WildFly) or Google App Engine.

Configure the `ThreadFactory` on the solver to create the <<multithreadedIncrementalSolving,move threads>>
and the <<partitionedSearch,Partition Search threads>> with it:

[source,xml,options="nowrap"]
----
<solver xmlns="https://timefold.ai/xsd/solver" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="https://timefold.ai/xsd/solver https://timefold.ai/xsd/solver/solver.xsd">
  <threadFactoryClass>...MyAppServerThreadFactory</threadFactoryClass>
  ...
</solver>
----


[#automaticNodeSharing]
=== Automatic node sharing

[NOTE]
====
This feature is a commercial feature of Timefold Solver Enterprise Edition.
It is not available in the Community Edition.
====

When a `ConstraintProvider` does an operation for multiple constraints (such as finding all shifts corresponding to an employee), that work can be shared.
This can significantly improve move evaluation speed if the repeated operation is computationally expensive:

image::enterprise-edition/nodeSharingValueProposition.png[align="center"]

==== Configuration

[tabs]
====
XML::

* Add `<constraintStreamAutomaticNodeSharing>true</constraintStreamAutomaticNodeSharing>` in your `solverConfig.xml`:
+
[source,xml,options="nowrap"]
----
<!-- ... -->
<scoreDirectorFactory>
  <constraintProviderClass>org.acme.MyConstraintProvider</constraintProviderClass>
  <constraintStreamAutomaticNodeSharing>true</constraintStreamAutomaticNodeSharing>
</scoreDirectorFactory>
<!-- ... -->
----

Spring Boot::

Set the property `timefold.solver.constraint-stream-automatic-node-sharing` to `true` in `application.properties`:
+
[source,properties,options="nowrap"]
----
timefold.solver.constraint-stream-automatic-node-sharing=true
----

Quarkus::

Set the property `quarkus.timefold.solver.constraint-stream-automatic-node-sharing` to `true` in `application.properties`:
+
[source,properties,options="nowrap"]
----
quarkus.timefold.solver.constraint-stream-automatic-node-sharing=true
----
====

[IMPORTANT]
====
To use automatic node sharing outside Quarkus, your `ConstraintProvider` class must oblige by several restrictions so a valid subclass can be generated:

- The `ConstraintProvider` class cannot be final.
- The `ConstraintProvider` class cannot have any final methods.
- The `ConstraintProvider` class cannot access any protected classes, methods or fields.

Debugging breakpoints put inside your constraints will not be respected, because the `ConstraintProvider` class will be transformed when this feature is enabled.
====

[#nodeSharing]
==== What is node sharing?

When using xref:constraints-and-score/score-calculation.adoc#constraintStreams[constraint streams], each xref:constraints-and-score/score-calculation.adoc#constraintStreamsBuildingBlocks[building block] forms a node in the score calculation network.
When two building blocks are functionally equivalent, they can share the same node in the network.
Sharing nodes allows the operation to be performed only once instead of multiple times, improving the performance of the solver.
To be functionally equivalent, the following must be true:

* The building blocks must represent the same operation.

* The building blocks must have functionally equivalent parent building blocks.

* The building blocks must have functionally equivalent inputs.

For example, the building blocks below are functionally equivalent:

[source,java,options="nowrap"]
----
Predicate<Shift> predicate = shift -> shift.getEmployee().getName().equals("Ann");

var a = factory.forEach(Shift.class)
               .filter(predicate);

var b = factory.forEach(Shift.class)
               .filter(predicate);
----

Whereas these building blocks are not functionally equivalent:

[source,java,options="nowrap"]
----
Predicate<Shift> predicate1 = shift -> shift.getEmployee().getName().equals("Ann");
Predicate<Shift> predicate2 = shift -> shift.getEmployee().getName().equals("Bob");

// Different parents
var a = factory.forEach(Shift.class)
               .filter(predicate2);

var b = factory.forEach(Shift.class)
               .filter(predicate1)
               .filter(predicate2);

// Different operations
var a = factory.forEach(Shift.class)
               .ifExists(Employee.class);

var b = factory.forEach(Shift.class)
               .ifNotExists(Employee.class);

// Different inputs
var a = factory.forEach(Shift.class)
               .filter(predicate1);

var b = factory.forEach(Shift.class)
               .filter(predicate2);
----

Counterintuitively, the building blocks produced by these (seemly) identical methods are not necessarily functionally equivalent:

[source,java,options="nowrap"]
----
UniConstraintStream<Shift> a(ConstraintFactory constraintFactory) {
    return factory.forEach(Shift.class)
                  .filter(shift -> shift.getEmployee().getName().equals("Ann"));
}

UniConstraintStream<Shift> b(ConstraintFactory constraintFactory) {
    return factory.forEach(Shift.class)
                  .filter(shift -> shift.getEmployee().getName().equals("Ann"));
}
----

The Java Virtual Machine is free to (and often does) create different instances of functionally equivalent lambdas.
This severely limits the effectiveness of node sharing, since the only way to know two lambdas are equal is to compare their references.

When automatic node sharing is used, the `ConstraintProvider` class is transformed so all lambdas are accessed via a static final field.
Consider the following input class:

[source,java,options="nowrap"]
----
public class MyConstraintProvider implements ConstraintProvider {

    public Constraint[] defineConstraints(ConstraintFactory constraintFactory) {
        return new Constraint[] {
            a(constraintFactory),
            b(constraintFactory)
        };
    }

    Constraint a(ConstraintFactory constraintFactory) {
        return factory.forEach(Shift.class)
                      .filter(shift -> shift.getEmployee().getName().equals("Ann"))
                      .penalize(SimpleScore.ONE)
                      .asConstraint("a");
    }

    Constraint b(ConstraintFactory constraintFactory) {
        return factory.forEach(Shift.class)
                      .filter(shift -> shift.getEmployee().getName().equals("Ann"))
                      .penalize(SimpleScore.ONE)
                      .asConstraint("b");
    }
}
----

When automatic node sharing is enabled, the class will be transformed to look like this:

[source,java,options="nowrap"]
----
public class MyConstraintProvider implements ConstraintProvider {
    private static final Predicate<Shift> $predicate1 = shift -> shift.getEmployee().getName().equals("Ann");

    public Constraint[] defineConstraints(ConstraintFactory constraintFactory) {
        return new Constraint[] {
            a(constraintFactory),
            b(constraintFactory)
        };
    }

    Constraint a(ConstraintFactory constraintFactory) {
        return factory.forEach(Shift.class)
                      .filter($predicate1)
                      .penalize(SimpleScore.ONE)
                      .asConstraint("a");
    }

    Constraint b(ConstraintFactory constraintFactory) {
        return factory.forEach(Shift.class)
                      .filter($predicate1)
                      .penalize(SimpleScore.ONE)
                      .asConstraint("b");
    }
}
----

This transformation means that debugging breakpoints placed inside the original `ConstraintProvider` will not be honored in the transformed `ConstraintProvider`.

From the above, you can see how this feature allows building blocks to share functionally equivalent parents, without needing the `ConstraintProvider` to be written in an awkward way.

[#constraintProfiling]
=== Constraint Profiling

[NOTE]
====
This feature is a commercial feature of Timefold Solver Enterprise Edition.
It is not available in the Community Edition.
====

Profiling allows you to identify which constraints are taking the majority of time spent in score calculation and thus are worth optimizing.
Unfortunately, traditional profiling tools only report the internal classes used to calculate constraints instead of the constraints themselves. xref:constraints-and-score/score-calculation.adoc#constraintStreams[Constraint streams] have builtin support for profiling constraints directly.

==== Configuration

[tabs]
====
XML::

* Add `<constraintStreamProfilingEnabled>true</constraintStreamProfilingEnabled>` in your `solverConfig.xml`:
+
[source,xml,options="nowrap"]
----
  <scoreDirectorFactory>
    <constraintProviderClass>...ConstraintProvider</constraintProviderClass>
    <constraintStreamProfilingEnabled>true</constraintStreamProfilingEnabled>
  </scoreDirectorFactory>
----

Spring Boot::

Set the property `timefold.solver.constraint-stream-profiling-enabled` to `true` in `application.properties`:
+
[source,properties,options="nowrap"]
----
timefold.solver.constraint-stream-profiling-enabled=true
----

Quarkus::

Set the property `quarkus.timefold.solver.constraint-stream-profiling-enabled` to `true` in `application.properties`:
+
[source,properties,options="nowrap"]
----
quarkus.timefold.solver.constraint-stream-profiling-enabled=true
----
====

Constraint profiling will log its results to the `ai.timefold.solver.enterprise.core.api.ConstraintProfiler` class at the end of solving.
For the report to be generated, xref:using-timefold-solver/running-the-solver.adoc#logging[the particular logging levels must be configured and enabled].

IMPORTANT: Constraint profiling is only supported for xref:constraints-and-score/score-calculation.adoc#constraintStreams[constraint stream score calculation]
when <<multithreadedSolving,multi-threaded solving>> is disabled.

==== Understanding the report

When constraint stream profiling is enabled, a breakdown of time spent in each constraint is logged at the end of solving.

[source,text]
----
INFO:  # Constraint Profiling Summary
|                                                    | Time Spent | Of Total |
| :------------------------------------------------- | ---------: | -------: |
| 2: Teacher time efficiency                         |     6.19 s |  23.15 % |
| 6: Room conflict                                   |     4.17 s |  15.61 % |
| 4: Student group conflict                          |     3.76 s |  14.07 % |
| 5: Teacher conflict                                |     3.41 s |  12.76 % |
| 1: Student group subject variety                   |     3.25 s |  12.14 % |
| 3: Teacher room stability                          |     3.25 s |  12.14 % |
| Work shared by 1, 2, 3, 4, 5, 6                    |     2.71 s |  10.12 % |

DEBUG: ## Constraint Profiling Node Breakdown
| 2: Teacher time efficiency                         | Time Spent | Of Total |
| :------------------------------------------------- | ---------: | -------: |
| JOIN Node 26                                       |     1.56 s |  25.20 % |
| JOIN Left Input 6                                  |     1.28 s |  20.69 % |
| JOIN Right Input 7                                 |     1.09 s |  17.55 % |
| FILTER 5                                           |     1.83 s |  29.49 % |
| SCORING 4                                          |     0.44 s |   7.06 % |

 - JOIN Node 26 defined at location org.acme.schooltimetabling.solver.TimetableConstraintProvider#teacherTimeEfficiency:77
 - JOIN Left Input 6 defined at location org.acme.schooltimetabling.solver.TimetableConstraintProvider#teacherTimeEfficiency:77
 - JOIN Right Input 7 defined at location org.acme.schooltimetabling.solver.TimetableConstraintProvider#teacherTimeEfficiency:77
 - FILTER 5 defined at location org.acme.schooltimetabling.solver.TimetableConstraintProvider#teacherTimeEfficiency:79
 - SCORING 4 defined at location org.acme.schooltimetabling.solver.TimetableConstraintProvider#teacherTimeEfficiency:84

...

TRACE: ## Constraint Profiling Lifecycle Breakdown
| 2: Teacher time efficiency               |  INSERT  |  UPDATE  |  RETRACT  |
| :--------------------------------------- | -------: | -------: | --------: |
| JOIN                                     |  13.12 % |  75.64 % |   11.23 % |
| FILTER                                   |  23.69 % |  50.45 % |   25.86 % |
| SCORING                                  |  13.10 % |  16.12 % |   70.78 % |

...
----

NOTE: The data is printed as a https://www.markdownguide.org/extended-syntax/#tables[Markdown table].
Any Markdown-enabled text editor will show a well-formatted table.

Add different logging levels, different information is reported:

INFO:: How much time spent in score calculation is spent by each constraint (total and relative percentage) in descending order.
+
[source,text]
----
| 2: Teacher time efficiency                         |     6.19 s |  23.15 % |
----
+
xref:enterprise-edition/enterprise-edition.adoc#nodeSharing[When node sharing] is used, the time spent will be reported with "Work shared by", with the numbers identifying constraints that shared the work.
In the example below, work was shared by constraints 1 to 6:
+
[source,text]
----
| Work shared by 1, 2, 3, 4, 5, 6                    |     2.71 s |  10.12 % |
----

`DEBUG`:: A breakdown for each constraint of time spent (total and relative percentage) by each constraint stream building block (e.g., join, filter).
+
[source,text]
----
| 2: Teacher time efficiency                         | Time Spent | Of Total |
| :------------------------------------------------- | ---------: | -------: |
| JOIN Node 26                                       |     1.56 s |  25.20 % |
| JOIN Left Input 6                                  |     1.28 s |  20.69 % |
| JOIN Right Input 7                                 |     1.09 s |  17.55 % |
| FILTER 5                                           |     1.83 s |  29.49 % |
| SCORING 4                                          |     0.44 s |   7.06 % |
----
+
To make it easier to map the building blocks to your `ConstraintProvider` code,
the location(s) where each building block was defined is also reported:
+
[source,text]
----
 - JOIN Node 26 defined at location org.acme.schooltimetabling.solver.TimetableConstraintProvider#teacherTimeEfficiency:77
----

`TRACE`:: A breakdown for each constraint of time spent by each building block,
shown as percentage of time spent in each of the lifecycle phases.
+
[source,text]
----
| 2: Teacher time efficiency               |  INSERT  |  UPDATE  |  RETRACT  |
| :--------------------------------------- | -------: | -------: | --------: |
| JOIN                                     |  13.12 % |  75.64 % |   11.23 % |
| FILTER                                   |  23.69 % |  50.45 % |   25.86 % |
| SCORING                                  |  13.10 % |  16.12 % |   70.78 % |
----
+
NOTE: This is only relevant to people intimately familiar with the underlying implementation of Constraint Streams
and can be safely ignored by everyday users.

==== Interpreting the results

Constraint profiling measures wall clock time, not CPU time,
and as such includes any garbage collection pauses that may occur during score calculation.
These numbers should be used as a guide to identify which constraints are the most expensive,
not as an exact measurement.

Due to the nature of the JVM, the results may vary between runs.
The solver needs to run for a sufficient amount of time
to allow the JVM to warm up and stabilize.
We recommend running the solver for minutes rather than seconds,
but more than that is unlikely to bring any additional benefit.
For increased confidence in the results,
you can run the solver multiple times in different JVM processes
and average the results.


[#throttlingBestSolutionEvents]
=== Throttling best solution events in `SolverManager`

[NOTE]
====
This feature is a commercial feature of Timefold Solver Enterprise Edition.
It is not available in the Community Edition.
====

This feature helps you avoid overloading your system with best solution events,
especially in the early phase of the solving process when the solver is typically improving the solution very rapidly.

To enable event throttling, use `ThrottlingBestSolutionEventConsumer` when starting a new `SolverJob` using `SolverManager`:

[source,java,options="nowrap"]
----
...
import ai.timefold.solver.enterprise.core.api.ThrottlingBestSolutionEventConsumer;
import java.time.Duration;
...

public class TimetableService {

    private SolverManager<Timetable, Long> solverManager;

    public String solve(Timetable problem) {
        var bestSolutionEventConsumer = ThrottlingBestSolutionEventConsumer.of(
            event -> {
               // Your custom event handling code goes here.
            },
            Duration.ofSeconds(1)); // Throttle to 1 event per second.

        String jobId = ...;
        solverManager.solveBuilder()
                .withProblemId(jobId)
                .withProblem(problem)
                .withBestSolutionEventConsumer(bestSolutionEventConsumer)
                .run(); // Start the solver job and listen to best solutions, with throttling.
        return jobId;
    }

}
----

This will ensure that your system will never receive more than one best solution event per second.
Some other important points to note:

- If multiple events arrive during the pre-defined 1-second interval, only the last event will be delivered.
- When the `SolverJob` terminates, the last event received will be delivered regardless of the throttle,
unless it was already delivered before.
- If your consumer throws an exception, we will still count the event as delivered.
- If the system is too occupied to start and execute new threads,
event delivery will be delayed until a thread can be started.

[NOTE]
====
If you are using the `ThrottlingBestSolutionEventConsumer` for intermediate best solutions
together with a final best solution consumer,
both these consumers will receive the final best solution.
====

[#multistageMoves]
=== Multistage Moves

[NOTE]
====
This feature is a commercial feature of Timefold Solver Enterprise Edition.
It is not available in the Community Edition.
====

Multistage moves are moves composed of one or more stages, where each stage selects a single `Move` to execute.

Each stage has access to either a `BasicVariableMoveEvaluator` or a `ListVariableMoveEvaluator`, which allows the stage to evaluate moves without executing them.

Stages are selected from either a `BasicVariableStageProvider` or a `ListVariableStageProvider`, which is initialized from the working solution at phase start.

Multistage moves are configured from either a xref:optimization-algorithms/move-selector-reference.adoc#multistageMoveSelector[MultistageMoveSelectorConfig]
or a xref:optimization-algorithms/move-selector-reference.adoc#listMultistageMoveSelector[ListMultistageMoveSelectorConfig].

Multistage moves are useful for creating specialized ruin-and-recreate moves where the valid values that won't violate hard constraints can be determined in advance.
